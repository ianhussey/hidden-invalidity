---
title: "Simplified example of how we implemented different assessments structural validity"
subtitle: "May be useful for educational purposes"
author: "Ian Hussey^[Ghent University. Email: ian.hussey@ugent.be]"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

As we note in the main manuscript of our Hidden Invalidity paper, our goal was to assess whether the widespread underreporting of metrics of structural validity is likely to represent merely the hidden validity of these scales or, more worryingly, hidden invalidity. 

Our goal is not to provide a set of prescriptive standards for others to use to assess structural validity. That said, reviewers of our manuscript suggested that a simplified analysis script may be useful to provide as supplementary materials for others' eductional benefits. 

Below we detail the metrics and cut-offs we employed. Users of this script must take great care in choosing their analytic strategy at each decision point - and there are many. While we employed cut-offs, many advocate that they not be used at all. 

# Example decision-making criteria

CFA

- Note somewhere that item reverse scoring has already been done; we are not interested in method factors such as reverse wordings, only interpretative factors.
- Fit indices
- Chi^2 assesses absolute fit. It is sample size dependant, and given our sample sizes will be significant in almost all cases and is therefore not informative. However, it is still important to report along with its degrees of freedom and associated p value (Kline, 2005; Hayduk et al, 2007). 
- Standardized Root Mean Residual (SRMR) assesses absolute fit too. 
- Tuckler-Lewis Fit Index (TLI) assesses Relative Fit. Compares to a null model, although is not a hypothesis test. Sample size independent, which is important here.
- Root Mean Square Error of Approximation (RMSEA) assesses Noncentrality.
- Bentlerâ€™s Comparative Fit Index (CLI) assesses Noncentrality.

- Hu and Bentler (1999) suggest a two-index presentation format of either SRMR with the CFI, TLI, or RMSEA. We'll report all four metrics and make a composite of all three two-index scores:
  - CLI >= .95 & SRMR <= .09
  - TFI >= .95 & SRMR <= .09
  - RMSEA <= .06 & SRMR <= .09
  - "We reccomend that practitioners use a cutoff value close to .95 for TLI (CFI) in combination with a cutoff value close to .09 for SRMR to evaluate model fit...A combination rule with RMSEA > .06 and SRMR > .09 (or .10) resulted in the least sum of Type I and II error rates (pps 27-28)."

Measurement Invariance tests

- MI via model fit indices (Chen, 2007; Putnick & Bornstein, 2016; see also Cheung & Rensvold, 2002; Meade et al., 2008).
- Configural invariance failure: same as CFA above.
- Metric and scalar invariance failure: delta_RMSEA > 0.01 or delta_CFI < -0.015

Overall scale evaluations

- For a scale to be rated "good" in our paper, it must demonstrate good internally consistency, stable across time (both immediately and over time), it must conform to the expected factor structure, and it must be invariant across age and gender.
- Criterion: omega_t >= .7 & dependability >= .7 & stability >= .7 & CFA fit >= Mixed & MI passed for both median age and gender

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      cache=TRUE,
                      cache.lazy=TRUE)
```

```{r}

# dependencies
library(tidyverse)
library(lavaan)
library(semTools)
library(knitr)
library(kableExtra)
library(moments)
library(plotrix)
library(lubridate)
library(psych)
library(rsample)
library(purrr)
library(ggalluvial)

# functions
# rounds all numeric variables in a dataframe to the desired number of places. Non-numeric variables will be ignored.
round_df <- function(df, digits) {
  nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
  df[,nums] <- round(df[,nums], digits = digits)
  (df)
}

# options
options(knitr.table.format = "html") # necessary configuration of tables

# disable scientific notation
options(scipen = 999) 

```

# Data

## Get data

For illustration, I take with complete data on the Need for Cognition scale from the AIID dataset. I include only participants who completed the NFC at two timepoints. Item level data represents T1 responses, with sum scores coming from their T1 and T2 responses. 

```{r}

# # get data
# load("../data/trimmed_data.RData")
# 
# example_data_nfc <- trimmed_data %>%
#   filter(individual_differences_measure == "NFC") %>%
#   mutate(median_age = ifelse(age > median(age, na.rm = TRUE), "high", "low")) %>%
#   select(user_id, session_id, datetime_ymdhms,
#          sex, age, median_age,
#          nfc1, nfc2, nfc3, nfc4, nfc5, nfc6, nfc7, nfc8, nfc9,
#          nfc10, nfc11, nfc12, nfc13, nfc14, nfc15, nfc16, nfc17, nfc18, individual_differences_sum_score)
# 
# # separate out user_ids with two timepoints per scale
# user_ids_with_two_timepoints_per_scale <- example_data_nfc %>%
#   count(user_id) %>%
#   filter(n >= 2)
# 
# # semi join with full data
# two_timepoints_per_scale <- example_data_nfc %>%
#   semi_join(user_ids_with_two_timepoints_per_scale,
#             by = "user_id") %>%
#   arrange(user_id, datetime_ymdhms) %>%
#   mutate(timepoint = ifelse(user_id == lag(user_id, n = 2), 3,
#                             ifelse(user_id == lag(user_id, n = 1), 2 , 1)),
#          # lag can't operate on the first row, so mung a solution to this.
#          timepoint = ifelse(is.na(timepoint), 1, timepoint)) %>%
#   filter(timepoint %in% c(1, 2)) %>%
#   dplyr::select(user_id, datetime_ymdhms, timepoint, individual_differences_sum_score)
# 
# # rehape t1
# timepoint_1 <- two_timepoints_per_scale %>%
#   filter(timepoint == 1) %>%
#   rename(individual_differences_sum_score_t1 = individual_differences_sum_score,
#          datetime_ymdhms_t1 = datetime_ymdhms) %>%
#   dplyr::select(-timepoint)
# 
# # reshape t2
# timepoint_2 <- two_timepoints_per_scale %>%
#   filter(timepoint == 2) %>%
#   rename(individual_differences_sum_score_t2 = individual_differences_sum_score,
#          datetime_ymdhms_t2 = datetime_ymdhms) %>%
#   dplyr::select(-timepoint)
# 
# # join
# two_timepoint_scores_temp <- timepoint_1 %>%
#   left_join(timepoint_2, by = "user_id") %>%
#   mutate(days_to_followup = as.duration(interval(start = ymd_hms(datetime_ymdhms_t1),
#                                                  end = ymd_hms(datetime_ymdhms_t2))),
#          days_to_followup = round(as.numeric(days_to_followup, "days"), 3)) %>%
#   na.omit
# 
# 
# # combine with item level data
# example_data <- two_timepoint_scores_temp %>%
#   left_join(example_data_nfc, by = c("user_id" = "user_id", "datetime_ymdhms_t1" = "datetime_ymdhms")) %>%
#   rename(sum_score_t2 = individual_differences_sum_score_t2,
#          sum_score = individual_differences_sum_score) %>%
#   mutate(Scale = "Need for Cognition") %>%
#   select(Scale, user_id, sex, age, median_age,
#          sum_score, sum_score_t2, days_to_followup,
#          nfc1, nfc2, nfc3, nfc4, nfc5, nfc6, nfc7, nfc8, nfc9,
#          nfc10, nfc11, nfc12, nfc13, nfc14, nfc15, nfc16, nfc17, nfc18)
# 
# # write to disk
# write_csv(example_data, "data/example_data.csv")

# read from disk
example_data <- read.csv("data/example_data.csv")

```

# Measurement model

The simplest measurement model of a single latent variable and no item covariance represents the model that most researchers tacitly assume when they calculate sum scores. We therefore employed this model for each of our scales on the basis that it is often (implicitly) the most common model employed in research using a given scale.

```{r}

nfc_model <- "NFC =~ nfc1 + nfc2 + nfc3 + nfc4 + nfc5 + nfc6 + nfc7 + nfc8 + nfc9 + 
                     nfc10 + nfc11 + nfc12 + nfc13 + nfc14 + nfc15 + nfc16 + nfc17 + nfc18" 

```

# Demographics

```{r}

n_per_scale <- example_data %>%
  count(Scale)

example_data %>%
  count() %>%
  rename(`N unique experimental sessions` = n) %>%
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

demographics_temp <- example_data %>%
  mutate(sex = recode(sex,
                      "f" = 1,
                      "m" = 2)) %>%
  group_by(user_id) %>%
  summarize(mean_age = mean(age),
            sex = mean(sex)) %>%
  ungroup() %>%
  mutate(sex = recode(sex,
                      "1" = "f",
                      "2" = "m"))

demographics_temp %>%
  count() %>%
  rename(`N unique participants` = n) %>%
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

demographics_temp %>%
  count(sex) %>%
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

demographics_temp %>%
  summarise(age_mean = mean(mean_age),
            age_median = median(mean_age),
            age_sd = sd(mean_age)) %>%
  round_df(2) %>%
  gather() %>%
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Structural validity analyses

## Distributions

```{r}

# scales
ggplot(example_data) + 
  geom_density(aes(sum_score)) 

```

```{r}

## distribution info
distributions_auto <- function(data, scale_name) {
  
  require(moments)
  require(plotrix)
  
  data %>%
    mutate(sum = rowSums(.)) %>%
    dplyr::select(sum) %>%
    summarize(M = round(mean(sum, na.rm = TRUE), 2),
              SE = round(std.error(sum, na.rm = TRUE), 2),
              SD = round(sd(sum, na.rm = TRUE), 2),
              Skewness = round(skewness(sum), 2),
              Kurtosis = round(kurtosis(sum), 2)) %>%
    mutate(Scale = scale_name)
}

dist_results <- example_data %>% 
  dplyr::select(nfc1, nfc2, nfc3, nfc4, nfc5, nfc6, nfc7, nfc8, nfc9, 
                nfc10, nfc11, nfc12, nfc13, nfc14, nfc15, nfc16, nfc17, nfc18) %>% 
  distributions_auto(., scale_name = "Need for Cognition")

```

## Internal consistency

```{r}

# needed for both consistency and cfa analyses
fit_cfa <- function(data, model) {
  require(lavaan)
  require(tidyverse)

  data %>%
    dplyr::select(-sex, -age) %>%
    cfa(model,
        data = .,
        order = c("1", "2", "3", "4", "5", "6"),
        estimator = "WLSMV")  # can notionally change this to Maximum Likelihood (ML) but produces worse fits across measures
}

```

```{r}

# the number of bootstraps
n_boots <- 100 ## NB this should be increased to 1000 or even 2000 for publication, but this greatly increases the execution time of the code. For this illustrative example, we'll set it to just 100. This is relatively fine to estimate medians/means, but the 95% CIs are likely to be poorly estimated.


# NB the below code isn't as elegant as I'd like, but works just fine. `model` should ideally be passed as an argument among the three nested functions, but I couldn't work out how to get purrr::map() to take an argument for the function it applies to the bootstraps. Instead, model is defined as a global variable before each estimate_reliability() call. This variable scoping is fine statistically, but represents suboptimal coding afaik.

# helper function
bootstrap_reliability_helper <- function(split, ...) {
  
  require(lavaan)
  require(semTools)
  require(tidyverse)
  
  fit_cfa(analysis(split), model) %>%
    reliability() %>%
    as.data.frame() %>%
    rownames_to_column(var = "metric") %>%
    select(metric, total) %>%
    filter(metric %in% c("alpha",
                         "omega2",
                         "omega3")) %>%
    mutate(metric = recode(metric,
                           "alpha" = "alpha",
                           "omega2" = "omega_t",
                           "omega3" = "omega_h"))
  
}

# apply bootstrapping, then extract coefficients via percentile method
estimate_reliability <- function(data, scale, ...) {
  
  require(purrr)
  require(rsample)
  require(tidyverse)
  require(timesavers)
  
  # create bootstraps using out of bag method. makes a df with values that are collapsed dfs.
  boot_samples <- data %>%
    bootstraps(., times = n_boots)
  
  # apply to each bootstrap
  boots <- boot_samples %>%
    mutate(reliability_results = map(splits, bootstrap_reliability_helper)) %>%
    unnest(reliability_results)
  
  # find CIs using percentile method
  boot_estimates <- boots %>%
    group_by(metric) %>%
    summarize(median   = quantile(total, 0.500),
              ci_lower = quantile(total, 0.025),
              ci_upper = quantile(total, 0.975)) %>%
    round_df(3) %>%
    mutate(Scale = scale)
  
  return(boot_estimates)
}

model <- nfc_model
consistency_estimates_temp <- estimate_reliability(example_data, "Need for Cognition")

# reshape
consistency_estimates_alpha <- consistency_estimates_temp %>%
  filter(metric == "alpha") %>%
  select(-metric) %>%
  rename(alpha = median,
         alpha_ci_lower = ci_lower,
         alpha_ci_upper = ci_upper)

consistency_estimates_omega_t <- consistency_estimates_temp %>%
  filter(metric == "omega_t") %>%
  select(-metric) %>%
  rename(omega_t = median,
         omega_t_ci_lower = ci_lower,
         omega_t_ci_upper = ci_upper)

consistency_estimates_omega_h <- consistency_estimates_temp %>%
  filter(metric == "omega_h") %>%
  select(-metric) %>%
  rename(omega_h = median,
         omega_h_ci_lower = ci_lower,
         omega_h_ci_upper = ci_upper)

consistency_results <- consistency_estimates_alpha %>%
  left_join(consistency_estimates_omega_t, by = "Scale") %>%
  left_join(consistency_estimates_omega_h, by = "Scale")

```

## Test-retest reliability

### Dependability

```{r}

dependability_data <- example_data %>%
  filter(days_to_followup < 1/24)  # retest within an hour

# # n per scale
n_per_scale_dependability <- dependability_data %>%
  count(Scale) %>%
  rename(`Test-retest dependability n` = n)

pearson_r_estimate <- function(data, Scale){
  require(tidyverse)
  require(psych)
  
  res <- data %>%
    select(sum_score, sum_score_t2) %>%
    cor.ci(n.iter = 1000, method = "pearson", plot = FALSE)
  
  results <-
    data.frame(Scale = Scale,
               pearson_r = res$means,
               pearson_r_ci_lwr = res$ci$low.e,
               pearson_r_ci_upr = res$ci$up.e) %>%
    round_df(3)
  
  return(results)
}

dependability_results <- pearson_r_estimate(data = dependability_data, Scale = "Need for Cognition") %>%
  rename(`Test-retest dependability r` = pearson_r,
         `dependability r lower` = pearson_r_ci_lwr,
         `dependability r upper` = pearson_r_ci_upr) %>%
  full_join(n_per_scale_dependability, by = "Scale") 

```

### Stability

```{r}

stability_data <- example_data %>%
  filter(days_to_followup >= 1 & days_to_followup < 365)  # retest more than a day and less than a year later

# n per scale
n_per_scale_stability <- stability_data %>%
  count(Scale) %>%
  rename(`Test-retest stability n` = n)

stability_results <-pearson_r_estimate(data = stability_data, Scale = "Need for Cognition") %>%
  rename(`Test-retest stability r` = pearson_r,
         `stability r lower` = pearson_r_ci_lwr,
         `stability r upper` = pearson_r_ci_upr) %>%
  full_join(n_per_scale_stability, by = "Scale") 

```

## CFA fits

```{r}

## fit a CFA and return its model fit statistics
cfa_fit_metrics <- function(fit, scale) {
  
  require(lavaan)
  require(tidyverse)
  
  # fit statistics
  fit_stats <- data.frame(estimate = fitMeasures(fit)) %>%
    rownames_to_column(var = "metric") %>%
    filter(metric %in% c("chisq", "df", "pvalue", "srmr", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper", "cfi", "tli")) %>%
    spread(metric, estimate) %>%
    mutate(Scale = scale,
           pvalue = ifelse(pvalue < .0001, "< .0001", as.character(round(pvalue, 4))),
           "chisq/df"  = round(chisq/df, 1),
           cfa_test = ifelse(srmr <= .09 & cfi >= .95 & tli >= .95 & rmsea <= .06, "Good",
                                ifelse(srmr <= .09 & (cfi >= .95 | tli >= .95 | rmsea <= .06), "Mixed", "Poor")),
           cfa_test = fct_relevel(cfa_test, "Good", "Mixed", "Poor"))
    
  return(fit_stats)
}

cfi_fit_results <- cfa_fit_metrics(fit_cfa(example_data, nfc_model), scale = "Need for Cognition") %>%
  round_df(3)

```

## Measurement Invariance

Between groups: gender and median age. 

Tests of measurement invariance (configural, metric, and scalar) using the cutoffs on the change in goodness-of-fit indices rather than a likelihood ratio test, which is highly senstive to sample size (Cheung & Rensvold, 2002). Cutoffs for configural invariance provided by Hu and Bentler (1999: same criteria as CFA fits above) and for metric and scalar invariance provided by Cheung & Rensvold (2002: Î”RMSEA <= 0.01 & Î”CLI <= -0.015; see also Chen, 2007 for very similar reccomendations). MI tests are conducted using the same methods as the CFAs: by assuming data is ordinal (e.g., likert data) using the distribution free DWLS estimator plus robust effect size estimates (i.e., estimator = "WLSMV").

```{r include=FALSE}

# i originally used semTools::measurementInvarianceCat() but it gave convergence issues with many models. it is also now depreciated. the following post suggested this has been observed before (https://groups.google.com/forum/#!topic/lavaan/aGCVepFrcCs). to maintain a uniform strategy across all scales, i changed to a manual CFA contraint strategy below.

mi_via_cutoffs <- function(data, model, scale, group) {
  
  # dependencies
  require(lavaan)
  require(tidyverse)
  require(semTools)
  require(timesavers)
  
  # fit the configural model 
  fit_configural <- data %>%
    cfa(model = model,
        data  = .,
        order = c("1", "2", "3", "4", "5", "6"),
        estimator = "WLSMV",
        group = group)
  
  # constrain slopes
  fit_metric <- data %>%
    cfa(model = model,
        data  = .,
        order = c("1", "2", "3", "4", "5", "6"),
        estimator = "WLSMV",
        group = group,
        group.equal = "loadings")
  
  # constrain intercepts
  fit_scalar <- data %>%
    cfa(model = model,
        data  = .,
        order = c("1", "2", "3", "4", "5", "6"),
        estimator = "WLSMV",
        group = group,
        group.equal = c("loadings", "intercepts"))
  
  # summarize these cfa fit indices, and their change across nested models
  results <- compareFit(fit_configural, fit_metric, fit_scalar, nested = TRUE) %>%
    summary()
  
  fit_indices <- results$fit.indices %>%
    rownames_to_column(var = "test") %>%
    mutate(MI_config = ifelse(srmr <= .09 & cfi.scaled >= .95 & 
                                tli.scaled >= .95 & 
                                rmsea.scaled <= .06, "Good",
                              ifelse(srmr <= .09 & (cfi.scaled >= .95 | 
                                                      tli.scaled >= .95 | 
                                                      rmsea.scaled <= .06), "Mixed", "Poor")),
           MI_config = ifelse(test == "fit_configural", MI_config, NA),
           pvalue.scaled = as.character(ifelse(pvalue.scaled < .0001, "< .0001", round(pvalue.scaled, 4)))) %>%
    round_df(3) %>%
    mutate(Scale = scale,
           group = group)
  
  fit_deltas <- results$fit.diff %>%
    rownames_to_column(var = "test") %>%
    mutate(MI_delta = ifelse(rmsea.scaled > 0.01 | cfi.scaled < -0.015, "Failed", "Passed")) %>%
    round_df(3) %>%
    mutate(Scale = scale,
           group = group)
  
  # return results as a list of data frames, including the fitted models
  return(list(fit_indices    = fit_indices,
              fit_deltas     = fit_deltas))
  
}

# mi for gender
MI_sex_nfc      <- mi_via_cutoffs(data = example_data, 
                                  model = nfc_model, 
                                  scale = "Need for Cognition", 
                                  group = "sex")

# mi for median age
MI_age_nfc      <- mi_via_cutoffs(data = example_data, 
                                  model = nfc_model, 
                                  scale = "Need for Cognition", 
                                  group = "median_age")

# combine
## fit indices
MI_combined_fit_indicies <-
  rbind(MI_sex_nfc$fit_indices,
        MI_age_nfc$fit_indices) %>%
  rename(df = df.scaled,
         fit_chisq_p_value = pvalue.scaled,
         chisq = chisq.scaled,
         srmr = srmr,
         cfi = cfi.scaled,
         tli = tli.scaled,
         rmsea = rmsea.scaled)

## change in fit indices
MI_combined_change_in_fit_indices <-
  rbind(MI_sex_nfc$fit_deltas,
        MI_age_nfc$fit_deltas) %>%
  mutate(test = ifelse(test == "fit_metric - fit_configural", "fit_metric",
                       ifelse(test == "fit_scalar - fit_scalar", "fit_scalar", NA))) %>%
  rename(df_change = df.scaled,
         srmr_change = srmr,
         cfi_change = cfi.scaled,
         tli_change = tli.scaled,
         rmsea_change = rmsea.scaled)

measurement_invariance_full_results <- MI_combined_fit_indicies %>%
  left_join(MI_combined_change_in_fit_indices, by = c("Scale", "group", "test")) %>%
  arrange(Scale, group, test) %>%
  mutate(MI_test = ifelse(!is.na(MI_config), MI_config, MI_delta)) %>%
  select(Scale, group, test, 
         chisq, df, fit_chisq_p_value, cfi, tli, rmsea, srmr, 
         df_change, cfi_change, tli_change, rmsea_change, srmr_change, MI_test) 


# summarize results in categorical terms
MI_results_temp_1 <- measurement_invariance_full_results %>%
  mutate(MI_test = ifelse(MI_test %in% c("Good", "Mixed"), "Passed",
                          ifelse(MI_test == "Poor", "Failed", MI_test))) %>%
  select(Scale, group, test, MI_test) %>%
  spread(test, MI_test) %>%
  mutate(MI_test = ifelse(fit_configural == "Passed" & 
                            fit_metric == "Passed" & 
                            fit_scalar == "Passed", "Passed", "Failed"),
         MI_test_failed = ifelse(fit_configural == "Failed", "Configural",
                                 ifelse(fit_metric == "Failed", "Metric",
                                        ifelse(fit_scalar == "Failed", "Scalar", NA))))

MI_results_temp_2 <- MI_results_temp_1 %>%
  select(Scale, group, MI_test) %>%
  spread(group, MI_test) %>%
  rename(MI_age = median_age,
         MI_sex = sex) %>%
  mutate(MI_test = ifelse(MI_age == "Passed" & MI_sex == "Passed", "Passed", "Failed")) 

MI_results_temp_3 <- MI_results_temp_1 %>%
  select(Scale, group, MI_test_failed) %>%
  spread(group, MI_test_failed) %>%
  rename(MI_age_test_failed = median_age,
         MI_sex_test_failed = sex)

MI_results <- full_join(MI_results_temp_2, MI_results_temp_3, by = "Scale") %>%
  select(Scale, MI_test, MI_age, MI_age_test_failed, MI_sex, MI_sex_test_failed)


# save full results to disk for reporting
write_csv(measurement_invariance_full_results, "measurement_invariance_full_results.csv")

```

# Combined table of results

```{r}

combined_results <- cfi_fit_results %>%
  left_join(dist_results,          by = "Scale") %>%
  left_join(consistency_results,   by = "Scale") %>%
  left_join(dependability_results, by = "Scale") %>%
  left_join(stability_results,     by = "Scale") %>%
  left_join(MI_results,            by = "Scale") %>%
  left_join(n_per_scale,           by = "Scale") %>%

  # apply cutoffs to reliability metrics
  dplyr::mutate(alpha_test   = ifelse(alpha >= .7,   "Passed", "Failed"),
                omega_t_test = ifelse(omega_t >= .7, "Passed", "Failed"),
                omega_h_test = ifelse(omega_h >= .7, "Passed", "Failed"),
                dependability_test = ifelse("Test-retest dependability r" >= .7, "Passed", "Failed"),
                stability_test     = ifelse("Test-retest stability r" >= .7,     "Passed", "Failed"),
                test_retest_test = ifelse(dependability_test == "Passed" & stability_test == "Passed", "Passed",
                                          ifelse(dependability_test == "Failed" & stability_test == "Failed", "Failed", NA)),
                overall_scale_test = ifelse(omega_t >= .7 &
                                              ("Test-retest dependability r" >= .7 |
                                                 is.na(`Test-retest dependability r`)) &
                                              ("Test-retest stability r" >= .7 |
                                                 is.na(`Test-retest stability r`)) &
                                              (cfa_test == "Good" | cfa_test == "Mixed") &
                                              MI_test == "Passed", "Good", "Questionable")) %>%
  # reorder columns
  dplyr::select(Scale, n,
                M, SD, Skewness, Kurtosis,
                # internal consistency
                alpha, alpha_ci_lower, alpha_ci_upper, alpha_test, 
                omega_t, omega_t_ci_lower, omega_t_ci_upper, omega_t_test,
                omega_h, omega_h_ci_lower, omega_h_ci_upper, omega_h_test,
                # test-retest reliability
                "Test-retest dependability n", "Test-retest dependability r", 
                "dependability r lower", "dependability r upper", dependability_test,
                "Test-retest stability n", "Test-retest stability r", 
                "stability r lower", "stability r upper", stability_test, test_retest_test,
                # cfa fit
                chisq, "chisq/df", df, pvalue,
                cfi, tli, rmsea, rmsea.ci.lower, rmsea.ci.upper, srmr, cfa_test, 
                # measurement invariance
                MI_age, MI_age_test_failed, MI_sex, MI_sex_test_failed, MI_test, 
                # summary
                overall_scale_test) %>%
  arrange(Scale)

# write to disk
write_csv(combined_results, "main_results_table.csv", na = "")

```

# Effect sizes for CFA and MI (mis)fit

A reviewer asked that we include a continuous effect size metric to quantify the impact of measurement (in)variance. We elected to also include metrics for the CFA models.  

## Correlations between latent and observed scores, and misclassification rate between them

i.e., effect sizes for CFA (mis)fit.

Magnitue of the correlations between latent scores and observed sum scores represent the (in)congruence between them. However, it is easy to misinterpret these correlations as representing near perfect just because they are "high" by the standards of bivariate relations (e.g., r >.95). In order to illustrate the impact of of such latent-observed incongruenties, we also dichotomize participants into groups based on mean latent/observed sum scores\*, and assess the rate of incongruent classifications (e.g., participants assigned to the high group on the latent scale but low group on the observed or vice versa). These illustrate that even "high" correlations such as *r* = .95 give rise to suprisingly high rates of incongruent classifications, which have many implications for the use of these scales. 

\*Of course, cut points for dichotomization are a subject of debate. Simply for illustrative purposes, we use the mean of the scale (i.e., 0 on the scaled and centered variable) to show the rate of incongruent conclusions when using the observed sum score vs. the latent mean for decision making.

```{r}

# fit models
nfc_fit <- fit_cfa(data = example_data, model = nfc_model)

# calculate latent and observed sum scores
latent_observed_nfc <- example_data %>%
  mutate(latent = as.data.frame(lavPredict(nfc_fit))$NFC) %>%
  rowwise() %>%
  mutate(observed = mean(c(nfc1, nfc2, nfc3, nfc4, nfc5, nfc6, nfc7, nfc8, nfc9, nfc10, nfc11, nfc12, nfc13, nfc14, nfc15, nfc16, nfc17, nfc18))) %>%
  ungroup() %>%
  select(age, sex, 
         latent, observed)


percent_misclassification <- function(data) {
  data %>%
    mutate(latent_std = as.numeric(scale(latent)),
           observed_std = as.numeric(scale(observed)),
           congruent_classification = ifelse(latent_std*observed_std > 0, TRUE, FALSE)) %>%
    filter(congruent_classification == FALSE) %>%
    summarize(percent = (n()/nrow(data))*100) %>%
    pull(percent)
}

latent_observed_correlation_results <- 
  data.frame(scale_name = c("Need for Cognition"),
             r = c(cor.test(latent_observed_nfc$latent,      latent_observed_nfc$observed)$estimate),
             r_ci_lwr = c(cor.test(latent_observed_nfc$latent,      latent_observed_nfc$observed)$conf.int[1]),
             r_ci_upr = c(cor.test(latent_observed_nfc$latent,      latent_observed_nfc$observed)$conf.int[2]),
             percent_misclassification = c(percent_misclassification(latent_observed_nfc))) %>%
  round_df(3) %>%
  mutate(percent_misclassification = round(percent_misclassification, 1)) 

# write to disk
write_csv(latent_observed_correlation_results, "effect_sizes_cfa_fits.csv")

# table
latent_observed_correlation_results %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

## Between-group differences on the latent vs. the observed scales

i.e., an effect size for measurement invariance.

```{r}

## fit a CFA and return its model fit statistics
fit_constrained_cfa <- function(data, model) {
  require(lavaan)
  require(tidyverse)
  
  # fit cfa
  cfa_data <- data %>%
    dplyr::select(-sex, -age)
  
  # sem model
  fit <- cfa(model, 
             data = cfa_data, 
             order = c("1", "2", "3", "4", "5", "6"),
             estimator = "WLSMV",
             group.equal = c("loadings", "intercepts")) 
  
  return(fit)
}

# fit models
nfc_fit_constrained <- fit_constrained_cfa(data = example_data, model = nfc_model)

# calculate latent and observed sum scores
constrained_latent_observed_nfc <- example_data %>%
  mutate(latent = as.data.frame(lavPredict(nfc_fit_constrained))$NFC) %>%
  rowwise() %>%
  mutate(observed = mean(c(nfc1, nfc2, nfc3, nfc4, nfc5, nfc6, nfc7, nfc8, nfc9, 
                           nfc10, nfc11, nfc12, nfc13, nfc14, nfc15, nfc16, nfc17, nfc18))) %>%
  ungroup() %>%
  select(median_age, sex, 
         latent, observed)

es_constrained_latent_age_nfc    <- effsize::cohen.d(latent ~   median_age, data = constrained_latent_observed_nfc)
es_constrained_latent_sex_nfc    <- effsize::cohen.d(latent ~   sex,        data = constrained_latent_observed_nfc)
es_constrained_observed_age_nfc  <- effsize::cohen.d(observed ~ median_age, data = constrained_latent_observed_nfc)
es_constrained_observed_sex_nfc  <- effsize::cohen.d(observed ~ sex,        data = constrained_latent_observed_nfc)

group_differences_results <- 
  data.frame(scale_name = c("Need for Cognition"),
             group_comparison = c("median age", "sex"),
             latent_cohens_d = c(es_constrained_latent_age_nfc$estimate,
                                 es_constrained_latent_sex_nfc$estimate),
             latent_cohens_d_ci_lwr = c(es_constrained_latent_age_nfc$conf.int[1],
                                        es_constrained_latent_sex_nfc$conf.int[1]),
             latent_cohens_d_ci_upr = c(es_constrained_latent_age_nfc$conf.int[2],
                                        es_constrained_latent_sex_nfc$conf.int[2]),
             observed_cohens_d = c(es_constrained_observed_age_nfc$estimate,
                                   es_constrained_observed_sex_nfc$estimate),
             observed_cohens_d_ci_lwr = c(es_constrained_observed_age_nfc$conf.int[1],
                                          es_constrained_observed_sex_nfc$conf.int[1]),
             observed_cohens_d_ci_upr = c(es_constrained_observed_age_nfc$conf.int[2],
                                          es_constrained_observed_sex_nfc$conf.int[2])) %>%
  mutate(observed_latent_cohens_d_diff = observed_cohens_d - latent_cohens_d,
         abs_observed_latent_cohens_d_diff = abs(observed_latent_cohens_d_diff),
         sig_change = ifelse((latent_cohens_d > observed_cohens_d_ci_lwr & 
                                latent_cohens_d > observed_cohens_d_ci_upr & 
                                observed_cohens_d < latent_cohens_d_ci_lwr & 
                                observed_cohens_d < latent_cohens_d_ci_upr) |
                               (latent_cohens_d < observed_cohens_d_ci_lwr & 
                                latent_cohens_d < observed_cohens_d_ci_upr & 
                                observed_cohens_d > latent_cohens_d_ci_lwr & 
                                observed_cohens_d > latent_cohens_d_ci_upr), TRUE, FALSE)) %>%
  round_df(3) %>%
  rename(Scale = scale_name) %>%
  select(Scale, group_comparison, latent_cohens_d, latent_cohens_d_ci_lwr, latent_cohens_d_ci_upr, 
         observed_cohens_d, observed_cohens_d_ci_lwr, observed_cohens_d_ci_upr, 
         observed_latent_cohens_d_diff, abs_observed_latent_cohens_d_diff, sig_change) 

# write to disk
write_csv(group_differences_results, "effect_sizes_measurement_invariance.csv")

# table
summary <- group_differences_results %>%
  select(Scale,
         group_comparison,
         observed_cohens_d,
         latent_cohens_d,
         abs_observed_latent_cohens_d_diff,
         sig_change)

summary %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```
